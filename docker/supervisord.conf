[supervisord]
nodaemon=true
user=root
logfile=/var/log/supervisor/supervisord.log
pidfile=/var/run/supervisord.pid

[program:ollama]
command=ollama serve
autostart=true
autorestart=true
stderr_logfile=/var/log/supervisor/ollama.err.log
stdout_logfile=/var/log/supervisor/ollama.out.log
environment=OLLAMA_HOST=0.0.0.0:11434,OLLAMA_KEEP_ALIVE=-1

[program:backend]
command=python3 /app/backend/main.py
directory=/app/backend
autostart=true
autorestart=true
stderr_logfile=/var/log/supervisor/backend.err.log
stdout_logfile=/var/log/supervisor/backend.out.log
environment=CUDA_VISIBLE_DEVICES="0,1",OLLAMA_KEEP_ALIVE=-1

[program:nginx]
command=nginx -g "daemon off;"
autostart=true
autorestart=true
stderr_logfile=/var/log/supervisor/nginx.err.log
stdout_logfile=/var/log/supervisor/nginx.out.log

[program:model-loader]
command=/app/load-models.sh
autostart=true
autorestart=false
startsecs=0
stderr_logfile=/var/log/supervisor/model-loader.err.log
stdout_logfile=/var/log/supervisor/model-loader.out.log